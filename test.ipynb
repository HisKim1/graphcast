{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from google.cloud import storage\n",
    "from graphcast import autoregressive\n",
    "from graphcast import casting\n",
    "from graphcast import checkpoint\n",
    "from graphcast import data_utils\n",
    "from graphcast import graphcast\n",
    "from graphcast import normalization\n",
    "from graphcast import rollout\n",
    "from graphcast import xarray_jax\n",
    "from graphcast import xarray_tree\n",
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import numpy as np\n",
    "import xarray\n",
    "import his_utils\n",
    "\n",
    "def parse_file_parts(file_name):\n",
    "  return dict(part.split(\"-\", 1) for part in file_name.split(\"_\"))\n",
    "\n",
    "gcs_client = storage.Client.create_anonymous_client()\n",
    "gcs_bucket = gcs_client.get_bucket(\"dm_graphcast\")\n",
    "gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the model and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization \n",
    "# TODO: 여기 값 바꾸면 됨 근데 모델 불러서 쓸거면 상관 없ㅅ음\n",
    "resolution = 0 # 0.25 or 1.0\n",
    "mesh_size = 1000 # 4~6\n",
    "latent_size = 100 # 2^4 ~ 2^9\n",
    "gnn_msg_steps = 10 # 1~32\n",
    "pressure_levels = 37 # 13, 25, 37\n",
    "hidden_layers = 1 \n",
    "radius_query_fraction_edge_length = 0.6 # 1로도 가능\n",
    "params = None\n",
    "state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model_config = graphcast.ModelConfig(\n",
    "        resolution=resolution, \n",
    "        mesh_size=mesh_size,\n",
    "        latent_size=latent_size,\n",
    "        gnn_msg_steps=gnn_msg_steps,\n",
    "        hidden_layers=hidden_layers,\n",
    "        radius_query_fraction_edge_length=radius_query_fraction_edge_length    \n",
    "    )\n",
    "\n",
    "task_config = graphcast.TaskConfig(\n",
    "        input_variables=(graphcast.TARGET_SURFACE_VARS + graphcast.TARGET_ATMOSPHERIC_VARS + graphcast.FORCING_VARS +\n",
    "        graphcast.STATIC_VARS),\n",
    "        target_variables=graphcast.TARGET_SURFACE_VARS + graphcast.TARGET_ATMOSPHERIC_VARS,\n",
    "        forcing_variables=graphcast.FORCING_VARS,\n",
    "        pressure_levels=graphcast.PRESSURE_LEVELS[pressure_levels],\n",
    "        input_duration=\"12h\"\n",
    "    )\n",
    "\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisits\n",
    "\n",
    "GC_original = 'GraphCast - ERA5 1979-2017 - resolution 0.25 - pressure levels 37 - mesh 2to6 - precipitation input and output.npz'\n",
    "GC_operational = 'GraphCast_operational - ERA5-HRES 1979-2021 - resolution 0.25 - pressure levels 13 - mesh 2to6 - precipitation output only.npz'\n",
    "GC_small = 'GraphCast_small - ERA5 1979-2015 - resolution 1.0 - pressure levels 13 - mesh 2to5 - precipitation input and output.npz'\n",
    "\n",
    "# TODO: 모델 하나 골라 로드하기\n",
    "pretrained_model = GC_original\n",
    "\n",
    "with gcs_bucket.blob(f\"params/{pretrained_model}\").open(\"rb\") as f:\n",
    "    ckpt = checkpoint.load(f, graphcast.CheckPoint)\n",
    "\n",
    "    \n",
    "params = ckpt.params  # 로드된 체크포인트에서 파라미터 가져오기\n",
    "state = {}  # 초기 상태는 빈 딕셔너리로 설정\n",
    "\n",
    "# 체크포인트에서 모델 구성 가져오기\n",
    "model_config = ckpt.model_config\n",
    "# 체크포인트에서 작업 구성 가져오기\n",
    "task_config = ckpt.task_config\n",
    "\n",
    "# 모델 설명 출력\n",
    "print(\"Model description:\\n\", ckpt.description, \"\\n\")\n",
    "# 모델 라이선스 출력\n",
    "print(\"Model license:\\n\", ckpt.license, \"\\n\")\n",
    "\n",
    "print(\"Model config:\\n\", model_config, \"\\n\")\n",
    "\n",
    "print(\"Task config:\\n\", task_config)\n",
    "\n",
    "# 모델과 데이터셋의 유효성을 검사하는 함수\n",
    "def data_valid_for_model(\n",
    "    file_name: str, \n",
    "    model_config: graphcast.ModelConfig, \n",
    "    task_config: graphcast.TaskConfig):\n",
    "    # 파일 이름에서 접미사 \".nc\"를 제거하고 파일의 각 부분을 파싱\n",
    "    file_parts = parse_file_parts(file_name.removesuffix(\".nc\"))\n",
    "    \n",
    "    # 모델의 설정과 데이터셋 파일의 설정을 비교하여 유효성 검사\n",
    "    return (\n",
    "        # 모델의 해상도가 0이거나 파일의 해상도와 일치해야 함\n",
    "        model_config.resolution in (0, float(file_parts[\"res\"])) and\n",
    "        # 모델의 압력 레벨 수가 파일의 압력 레벨 수와 일치해야 함\n",
    "        len(task_config.pressure_levels) == int(file_parts[\"levels\"]) and\n",
    "        (\n",
    "            # 모델이 강수량을 입력 변수로 가지고 있는 경우, 파일 소스는 \"era5\" 또는 \"fake\"여야 함\n",
    "            (\"total_precipitation_6hr\" in task_config.input_variables and\n",
    "             file_parts[\"source\"] in (\"era5\", \"fake\")) or\n",
    "            # 모델이 강수량을 입력 변수로 가지고 있지 않은 경우, 파일 소스는 \"hres\" 또는 \"fake\"여야 함\n",
    "            (\"total_precipitation_6hr\" not in task_config.input_variables and\n",
    "             file_parts[\"source\"] in (\"hres\", \"fake\"))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "file_name = 'testdata/2022-01-01/source-era5_date-2022-01-01_res-0.25_levels-37_steps-12.nc'\n",
    "\n",
    "dataset = xarray.open_dataset(file_name)\n",
    "\n",
    "print(\"TOA_solar_incident_radiation\" in dataset.data_vars)\n",
    "\n",
    "# dataset.drop_vars(\"TOA_solar_incident_radiation\") #       <---- TOA 사용 여부 변경!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점검\n",
    "\n",
    "# 데이터셋 파일이 모델 설정에 유효한지 확인\n",
    "# if not data_valid_for_model(file_name, model_config, task_config):\n",
    "    # raise ValueError(\n",
    "        # \"Invalid dataset file, rerun the cell above and choose a valid dataset file.\")\n",
    "# \n",
    "# 데이터셋의 시간 차원이 최소 3인지 확인 (입력용 2, 목표용 1 이상)\n",
    "# assert dataset.dims[\"time\"] >= 3  # 2 for input, >=1 for targets\n",
    "\n",
    "\n",
    "# 선택한 데이터셋 파일의 정보를 출력\n",
    "# print(\", \".join([f\"{k}: {v}\" for k, v in parse_file_parts(file_name.removesuffix(\".nc\")).items()]))\n",
    "\n",
    "# 로드된 데이터셋을 출력\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋의 시간 크기 확인\n",
    "total_time_steps = dataset.sizes[\"time\"] - 2\n",
    "\n",
    "total_time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "train_steps = 0\n",
    "eval_steps = 12\n",
    "\n",
    "# train_steps와 eval_steps가 유효한 범위 내에 있는지 확인\n",
    "# assert 1 <= train_steps <= total_time_steps, f\"train_steps must be between 1 and {total_time_steps}.\"\n",
    "\n",
    "# assert 1 <= eval_steps <= total_time_steps, f\"eval_steps must be between 1 and {total_time_steps}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs, train_targets, train_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "#     dataset, \n",
    "#     target_lead_times=slice(\"6h\", f\"{train_steps * 6}h\"), \n",
    "#     **dataclasses.asdict(task_config)\n",
    "# )\n",
    "\n",
    "eval_inputs, eval_targets, eval_forcings = data_utils.extract_inputs_targets_forcings(\n",
    "    dataset, \n",
    "    target_lead_times=slice(\"6h\", f\"{eval_steps * 6}h\"), \n",
    "    **dataclasses.asdict(task_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행\n",
    "\n",
    "데이터 뽑기는 여기 앞까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dimensions of the example batch and extracted training and evaluation data\n",
    "print(\"All Examples:  \", dataset.dims.mapping)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Train Inputs:  \", train_inputs.dims.mapping)\n",
    "# print(\"Train Targets: \", train_targets.dims.mapping)\n",
    "# print(\"Train Forcings:\", train_forcings.dims.mapping)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Eval Inputs:   \", eval_inputs.dims.mapping)\n",
    "print(\"Eval Targets:  \", eval_targets.dims.mapping)\n",
    "print(\"Eval Forcings: \", eval_forcings.dims.mapping)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Eval Inputs:   \\n\", eval_inputs)\n",
    "print(\"Eval Targets:  \\n\", eval_targets)\n",
    "print(\"Eval Forcings: \\n\", eval_forcings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load normalization data\n",
    "\n",
    "# Load the dataset containing the standard deviations of differences by level\n",
    "# from the specified Google Cloud Storage (GCS) bucket\n",
    "with gcs_bucket.blob(\"stats/diffs_stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    diffs_stddev_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "# Load the dataset containing the mean values by level\n",
    "# from the specified GCS bucket\n",
    "with gcs_bucket.blob(\"stats/mean_by_level.nc\").open(\"rb\") as f:\n",
    "    mean_by_level = xarray.load_dataset(f).compute()\n",
    "\n",
    "# Load the dataset containing the standard deviations by level\n",
    "# from the specified GCS bucket\n",
    "with gcs_bucket.blob(\"stats/stddev_by_level.nc\").open(\"rb\") as f:\n",
    "    stddev_by_level = xarray.load_dataset(f).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT 컴파일된 함수를 생성하고, 필요한 경우 랜덤 가중치를 초기화합니다\n",
    "\n",
    "def construct_wrapped_graphcast(\n",
    "    model_config: graphcast.ModelConfig,\n",
    "    task_config: graphcast.TaskConfig):\n",
    "  \"\"\"GraphCast 예측기를 구성하고 래핑합니다.\"\"\"\n",
    "  predictor = graphcast.GraphCast(model_config, task_config)\n",
    "  predictor = casting.Bfloat16Cast(predictor)\n",
    "  predictor = normalization.InputsAndResiduals(\n",
    "      predictor,\n",
    "      diffs_stddev_by_level=diffs_stddev_by_level,\n",
    "      mean_by_level=mean_by_level,\n",
    "      stddev_by_level=stddev_by_level)\n",
    "  predictor = autoregressive.Predictor(predictor, gradient_checkpointing=True)\n",
    "  return predictor\n",
    "\n",
    "@hk.transform_with_state\n",
    "def run_forward(model_config, task_config, inputs, targets_template, forcings):\n",
    "    \"\"\"구성된 예측기를 사용하여 순전파를 실행합니다.\"\"\"\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    return predictor(inputs, targets_template=targets_template, forcings=forcings)\n",
    "\n",
    "@hk.transform_with_state\n",
    "def loss_fn(model_config, task_config, inputs, targets, forcings):\n",
    "    \"\"\"구성된 예측기를 사용하여 손실과 진단 정보를 계산합니다.\"\"\"\n",
    "    predictor = construct_wrapped_graphcast(model_config, task_config)\n",
    "    loss, diagnostics = predictor.loss(inputs, targets, forcings)\n",
    "    return xarray_tree.map_structure(\n",
    "        lambda x: xarray_jax.unwrap_data(x.mean(), require_jax=True),\n",
    "        (loss, diagnostics)\n",
    "    )\n",
    "\n",
    "def grads_fn(params, state, model_config, task_config, inputs, targets, forcings):\n",
    "  \"\"\"매개변수에 대한 손실 함수의 그래디언트를 계산합니다.\"\"\"\n",
    "  def _aux(params, state, i, t, f):\n",
    "    (loss, diagnostics), next_state = loss_fn.apply(\n",
    "        params, state, jax.random.PRNGKey(0), model_config, task_config,\n",
    "        i, t, f)\n",
    "    return loss, (diagnostics, next_state)\n",
    "  (loss, (diagnostics, next_state)), grads = jax.value_and_grad(\n",
    "      _aux, has_aux=True)(params, state, inputs, targets, forcings)\n",
    "  return loss, diagnostics, next_state, grads\n",
    "\n",
    "def with_configs(fn):\n",
    "  \"\"\"모델 및 작업 구성을 적용하는 유틸리티 함수입니다.\"\"\"\n",
    "  return functools.partial(\n",
    "      fn, model_config=model_config, task_config=task_config)\n",
    "\n",
    "def with_params(fn):\n",
    "  \"\"\"함수가 항상 매개변수와 상태를 받도록 보장합니다.\"\"\"\n",
    "  return functools.partial(fn, params=params, state=state)\n",
    "\n",
    "def drop_state(fn):\n",
    "  \"\"\"예측만 반환합니다. 롤아웃 코드에 필요합니다.\"\"\"\n",
    "  return lambda **kw: fn(**kw)[0]\n",
    "\n",
    "init_jitted = jax.jit(with_configs(run_forward.init))\n",
    "\n",
    "if params is None:\n",
    "    # 매개변수와 상태가 아직 설정되지 않은 경우 초기화합니다.\n",
    "    params, state = init_jitted(\n",
    "        rng=jax.random.PRNGKey(0),\n",
    "        inputs=train_inputs,\n",
    "        targets_template=train_targets,\n",
    "        forcings=train_forcings\n",
    "    )\n",
    "\n",
    "# 필요한 구성과 매개변수로 함수들을 JIT 컴파일합니다.\n",
    "loss_fn_jitted = drop_state(with_params(jax.jit(with_configs(loss_fn.apply))))\n",
    "grads_fn_jitted = with_params(jax.jit(with_configs(grads_fn)))\n",
    "run_forward_jitted = drop_state(with_params(jax.jit(with_configs(run_forward.apply))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the model resolution matches the data resolution\n",
    "assert model_config.resolution in (0, 360. / eval_inputs.sizes[\"lon\"]), (\n",
    "    \"Model resolution doesn't match the data resolution. You likely want to \"\n",
    "    \"re-filter the dataset list, and download the correct data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_template = his_utils.create_target_dataset(time_steps=eval_steps, \n",
    "                   resolution=model_config.resolution, \n",
    "                   pressure_levels=len(task_config.pressure_levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcings = his_utils.create_forcing_dataset(time_steps=eval_steps,\n",
    "                                              resolution=model_config.resolution,\n",
    "                                              start_time=\"2021-01-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.__version__)\n",
    "print(jax.devices())\n",
    "\n",
    "import his_utils\n",
    "\n",
    "# Perform autoregressive rollout to generate predictions\n",
    "predictions = rollout.chunked_prediction(\n",
    "    run_forward_jitted,\n",
    "    rng=jax.random.PRNGKey(0),\n",
    "    inputs=eval_inputs,\n",
    "    targets_template=target_template,\n",
    "    forcings=forcings\n",
    ")\n",
    "\n",
    "# Display the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_netcdf(\"predictions_google_2022-01-01T00h_12step.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot\n",
    "\n",
    "이쁘게 포장하는 공정은 여기부터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 움짤 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import his_utils\n",
    "\n",
    "# 'figure' 디렉토리의 경로를 지정합니다.\n",
    "figure_dir = 'figure'\n",
    "\n",
    "# # 파일 이름에서 날짜와 시간을 추출하는 정규표현식\n",
    "# pattern = r'polar_GC_temperature_(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}).png'\n",
    "\n",
    "# # 파일 이름에서 날짜와 시간을 추출하여 리스트를 만듭니다.\n",
    "# date_list = sorted([re.search(pattern, f).group(1) for f in os.listdir(figure_dir) if f.startswith('polar_GC_temperature_') and f.endswith('.png')])\n",
    "\n",
    "# 정렬된 date_list를 사용하여 image_frames를 생성합니다.\n",
    "image_frames = [Image.open(os.path.join(figure_dir, f'total_precipitation_6hrRR_{date}.png')) for date in range(0,14)]\n",
    "\n",
    "# GIF를 저장합니다.\n",
    "his_utils.save_gif(image_frames, 'tp_6hr diff.gif', duration=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "list = [\n",
    " 'total_precipitation_6hr']\n",
    "\n",
    "def process_var(var, date_list, duration):\n",
    "    image_frames = []\n",
    "    for date in date_list:\n",
    "        with Image.open(os.path.join(\"figure/\", f'{var}_{date}.png')) as img:\n",
    "            image_frames.append(img.copy())\n",
    "    \n",
    "    his_utils.save_gif(image_frames, f'2021-01-01_{var}.gif', duration=duration)\n",
    "\n",
    "process_var_partial = partial(process_var, date_list=date_list, duration=700)\n",
    "\n",
    "# 멀티프로세싱 풀 생성 및 작업 실행\n",
    "with Pool() as pool:\n",
    "    pool.map(process_var_partial, list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "ERA5 = xr.open_dataset(f'testdata/2022-01-01RR.nc')\n",
    "google = xr.open_dataset('testdata/source-era5_date-2022-01-01_res-0.25_levels-37_steps-12.nc').drop_vars('toa_incident_solar_radiation')\n",
    "\n",
    "def plot(args):\n",
    "    dataset, target_var, time_index = args\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180))\n",
    "\n",
    "    data = dataset * 1000\n",
    "\n",
    "    im = ax.pcolormesh(data.lon, data.lat, data.isel(time=time_index).squeeze(), \n",
    "                   transform=ccrs.PlateCarree(), \n",
    "                   cmap='Blues',\n",
    "                   norm=TwoSlopeNorm(vmin=0, vcenter=0.5, vmax=20),\n",
    "                   shading='auto')\n",
    "    \n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    ax.gridlines(draw_labels=True)\n",
    "\n",
    "    flag = 'batch' in dataset.coords\n",
    "\n",
    "    plt.title(f'{flag} {target_var}\\nTime: {time_index}')\n",
    "    ax.set_global()\n",
    "\n",
    "    plt.savefig(f'figure/{flag} {target_var}_{time_index}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "with Pool() as pool:\n",
    "    args_list = [(ERA5['total_precipitation_6hr'], 'total_precipitation_6hr',time_index) for time_index in range(0, 14)]\n",
    "    pool.map(plot, args_list)\n",
    "\n",
    "with Pool() as pool:\n",
    "    args_list = [(google['total_precipitation_6hr'], 'total_precipitation_6hr', time_index) for time_index in range(0, 14)]\n",
    "    pool.map(plot, args_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hiskim1_graphcast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
